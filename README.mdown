# SSE matrix 4x4 multiplication benchmark

Based on article: [4x4 float matrix multiplication using SSE intrinsics](http://fhtr.blogspot.com/2010/02/4x4-float-matrix-multiplication-using.html)

### Usage

`make O=(2,3) TYPE=(0,1,2) CC=(gcc-4.6.2,clang-3.0,cc) ITER=100000000`

### Results 100M iterations

Tested on:

* **Mac OS X 10.7.3**
* **Intel Core i5 2400 2.5 GHz**

Results:

	           version  opt (-Ox)   plain  naive SSE    opt SSE
	     GCC [1] 4.7           2     3.24       4.40       1.90
	     GCC     4.7           3     0.75       2.13       0.54
	     GCC [1] 4.6.2         2     3.11       4.17       1.66
	     GCC     4.6.2         3     0.77       2.13       0.59
	   clang [2] 3.1/318.0.45  2     2.99       3.42       1.04
	   clang     3.1/318.0.45  3     3.03       3.42       1.04
	   clang [3]   3.0         2     2.99       3.42       1.04
	   clang       3.0         3     3.03       3.40       1.00
	GCC/LLVM [4] 2.4.1/2336.1  2     2.60       5.03      crash
	GCC/LLVM     2.4.1/2336.1  3     2.59       5.04      crash
	     GCC [5] 2.4.1         2     2.98       4.62       1.93
	     GCC     2.4.1         3     2.70       4.41       1.63

[1] *GCC 4.6.2* was built from [GNU sources](http://gcc.gnu.org/).

[2] *clang 3.1* binaries provided by *Xcode 4.3* package.

[3] *clang 3.0* was built from sources provided by [LLVM project](http://llvm.org/).

[4] *GCC/LLVM 2.4.1* binaries provided by *Xcode 4.2* package.

[5] *GCC 2.4.1* binaries provided by *Xcode 3.2.6* package.

## Why Clang sucks in comparison to GCC

*GCC* can perform whole loop of operations using XMM (SSE) registers only. *Clang* moves values back and forth between generic and XMM registers.

*Clang* 3.1 loop:

	LBB0_5:                                 ##   Parent Loop BB0_4 Depth=1
	                                        ## =>  This Inner Loop Header: Depth=2
		movss	-96(%rbp,%rcx,4), %xmm4
		pshufd	$0, %xmm4, %xmm4        ## xmm4 = xmm4[0,0,0,0]
		mulps	%xmm3, %xmm4
		movss	-92(%rbp,%rcx,4), %xmm5
		pshufd	$0, %xmm5, %xmm5        ## xmm5 = xmm5[0,0,0,0]
		mulps	%xmm0, %xmm5
		addps	%xmm4, %xmm5
		movss	-88(%rbp,%rcx,4), %xmm4
		pshufd	$0, %xmm4, %xmm4        ## xmm4 = xmm4[0,0,0,0]
		mulps	%xmm1, %xmm4
		addps	%xmm5, %xmm4
		movss	-84(%rbp,%rcx,4), %xmm5
		pshufd	$0, %xmm5, %xmm5        ## xmm5 = xmm5[0,0,0,0]
		mulps	%xmm2, %xmm5
		addps	%xmm4, %xmm5
		movaps	%xmm5, -192(%rbp,%rcx,4)
		leaq	4(%rcx), %rcx
		cmpl	$16, %ecx
		jl	LBB0_5

*GCC* 4.6.2 loop:

	L6:
		movaps	%xmm2, %xmm9
	L3:
		movaps	%xmm1, %xmm8
		movaps	%xmm9, %xmm4
		movaps	%xmm0, %xmm2
		mulps	%xmm3, %xmm8
		movaps	%xmm6, %xmm5
		addq	$1, %rdx
		mulps	%xmm3, %xmm4
		cmpq	%rax, %rdx
		mulps	%xmm10, %xmm2
		mulps	%xmm3, %xmm5
		movaps	%xmm8, %xmm7
		addps	%xmm4, %xmm7
		addps	%xmm4, %xmm1
		movaps	%xmm0, %xmm4
		movaps	%xmm8, %xmm0
		mulps	%xmm3, %xmm4
		addps	%xmm9, %xmm0
		addps	%xmm7, %xmm2
		addps	%xmm4, %xmm1
		addps	%xmm4, %xmm0
		addps	%xmm7, %xmm4
		addps	%xmm5, %xmm2
		addps	%xmm5, %xmm1
		addps	%xmm5, %xmm0
		addps	%xmm4, %xmm6
		jl	L6
		movaps	%xmm2, (%rsp)
		movaps	%xmm1, 16(%rsp)
		movaps	%xmm0, 32(%rsp)
		movaps	%xmm6, 48(%rsp)
